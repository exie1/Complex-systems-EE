{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numpy import random\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "# import levy\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gittins index\n",
    "Implementing Bernoulli armed bandits, and finding the optimal solution via Gittins index computation. Each (?) bandit has a chance of terminating at any given step, and the properties of each bandit can only be altered when sampling from the bandit.\n",
    "Gittens index is calculated as:\n",
    "$$ \\nu(i) = \\sup_{\\tau>0} \\frac{\\langle \\sum_{t=0}^{\\tau-1} \\beta^t R[Z(t)] \\rangle_{Z(0)=i} }{\\langle \\sum_{t=0}^{\\tau-1} \\beta^t\\rangle_{Z(0)=i}}, $$\n",
    "where $\\tau$ is the stopping time, $\\beta$ is a discounting parameter (could use probability of non-termination/step), $R[i]$ is the specific reward/payoff function allocated to the bandit, $Z(t)$ is a stochastic process within the bandit determining the chance of payoff, and $\\langle \\cdot \\rangle$ is the conditional expectation given that specific bandit/state.\n",
    "Alternatively you can view this as the discounted reward (up to $\\tau$) normalised by the discounted time (up to $\\tau$). \n",
    "\n",
    "Here we aim to optimise the stopping time (supremum). So to play at a multi-armed bandit game, you pick the option with the highest GI at the given state, and play it until the optimal stopping time, upon which we swap to the next option with the highest GI. This works because the states of the other options are fixed, since they are not being sampled.\n",
    "\n",
    "## Calibration\n",
    "To determine the Gittins index of a bandit, consider a one-armed bandit problem, where we have the option of pulling the arm for potential reward (Bernoulli distributed). Here we also provide the option of retirement, where you can opt out of the risky arm for some guaranteed payoff (safe arm). The GI of the risky arm is the required level of payoff provided by the safe arm such that an agent will initially pick between the arms indifferently.\n",
    "\n",
    "To calculate GI, we can use a binary search algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65c852718bb5874aa9e183b950236deed7089fd90bc90d9ee9badd03d8eb83c9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('env_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
